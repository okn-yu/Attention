{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Attention to DateTranslation Datasets.\n",
    "* Attention works prettye well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_date = [] # 変換前の日付データ\n",
    "output_date = [] # 変換後の日付データ\n",
    "\n",
    "file_path = 'data/date.txt'\n",
    "\n",
    "# date.txtを1行ずつ読み込んで変換前と変換後に分割して、inputとoutputで分ける\n",
    "with open(file_path, \"r\") as f:\n",
    "    date_list = f.readlines()\n",
    "    for date in date_list:\n",
    "        date = date[:-1]\n",
    "        input_date.append(date.split(\"_\")[0])\n",
    "        output_date.append(\"_\" + date.split(\"_\")[1])\n",
    "\n",
    "# inputとoutputの系列の長さを取得\n",
    "# すべて長さが同じなので、0番目の要素でlenを取ってます\n",
    "input_len = len(input_date[0]) # 29\n",
    "output_len = len(output_date[0]) # 10\n",
    "\n",
    "# date.txtで登場するすべての文字にIDを割り当てる\n",
    "char2id = {}\n",
    "for input_chars, output_chars in zip(input_date, output_date):\n",
    "    for c in input_chars:\n",
    "        if not c in char2id:\n",
    "            char2id[c] = len(char2id)\n",
    "    for c in output_chars:\n",
    "        if not c in char2id:\n",
    "            char2id[c] = len(char2id)\n",
    "\n",
    "input_data = [] # ID化された変換前日付データ\n",
    "output_data = [] # ID化された変換後日付データ\n",
    "for input_chars, output_chars in zip(input_date, output_date):\n",
    "    input_data.append([char2id[c] for c in input_chars])\n",
    "    output_data.append([char2id[c] for c in output_chars])\n",
    "\n",
    "# 7:3でtrainとtestに分ける\n",
    "train_x, test_x, train_y, test_y = train_test_split(input_data, output_data, train_size= 0.7)\n",
    "\n",
    "# データをバッチ化するための関数を定義\n",
    "def train2batch(input_data, output_data, batch_size=100):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    input_shuffle, output_shuffle = shuffle(input_data, output_data)\n",
    "    for i in range(0, len(input_data), batch_size):\n",
    "        input_batch.append(input_shuffle[i:i+batch_size])\n",
    "        output_batch.append(output_shuffle[i:i+batch_size])\n",
    "    return input_batch, output_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 200\n",
    "hidden_dim = 128\n",
    "vocab_size = len(char2id)\n",
    "\n",
    "BATCH_NUM = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder\n",
    "* LSTM has cell-state, but GRU does not have cell-state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=char2id[\" \"])\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        embedding = self.word_embeddings(sequence)\n",
    "        hs, h = self.gru(embedding)\n",
    "        # GRU has no cell-state!\n",
    "        return hs, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size, num_layers=1):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=char2id[\" \"])\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # hidden_dim * 2: concat GRU hidden dim and Attention context-vector.\n",
    "        self.hidden2linear = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "        # dim=1 is input_time-sequence.\n",
    "        self.softmax = nn.Softmax(dim=1) \n",
    "\n",
    "    def forward(self, sequence, hs, h):\n",
    "        embedding = self.word_embeddings(sequence)\n",
    "        output, state = self.gru(embedding, h)\n",
    "        \n",
    "        # Attention layer.\n",
    "        # t_output format: [batch_size, emb_vec, output_time_seq]\n",
    "        t_output = torch.transpose(output, 1, 2)\n",
    "        \n",
    "        # hs_format: [batch_size, input_time_seq, emb_vec]\n",
    "        # s_format: [batch_size, input_time_seq, output_time_seq]\n",
    "        s = torch.bmm(hs, t_output)\n",
    "        \n",
    "        # atteintion_weight.shape == s.shape\n",
    "        attention_weight = self.softmax(s)\n",
    "        \n",
    "        # c_format: [batch_size, 1, emb_vec]\n",
    "        c = torch.zeros(self.batch_size, 1, self.hidden_dim, device=device)\n",
    "        \n",
    "        for i in range(attention_weight.size()[2]):\n",
    "            # unsq_weight: [batch_size, input_time_seq, 1]\n",
    "            unsq_weight = attention_weight[:, :, i].unsqueeze(2)\n",
    "            # hs_format: [batch_size, input_time_seq, emb_vec]\n",
    "            # weited_hs_format: [batch_size, input_time_seq, emb_vec]\n",
    "            weighted_hs = hs * unsq_weight\n",
    "            # weight_sum_format: [batch_size, 1, emb_vec]\n",
    "            weight_sum = torch.sum(weighted_hs, axis=1).unsqueeze(1)\n",
    "            # c_format: [batch_size, i, emb_vec]\n",
    "            c = torch.cat([c, weight_sum], dim=1)\n",
    "            \n",
    "        # rm zero elm.\n",
    "        c = c[:, 1:, :]\n",
    "        #print(\"output.shape, c.shape: \", output.shape, c.shape)\n",
    "        output = torch.cat([output, c], dim=2)\n",
    "        output = self.hidden2linear(output)\n",
    "\n",
    "        return output, state, attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(EPOCH_NUM=100, lr=0.001):\n",
    "    all_losses = []\n",
    "    print(\"training...\")\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(1, EPOCH_NUM + 1):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        input_batch, output_batch = train2batch(train_x, train_y, batch_size=BATCH_NUM)\n",
    "\n",
    "        for i in range(len(input_batch)):\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            input_ts = torch.tensor(input_batch[i], device=device)\n",
    "            output_ts = torch.tensor(output_batch[i], device=device)\n",
    "\n",
    "            hs, h = encoder(input_ts)\n",
    "\n",
    "            # last-data can not be next-input.\n",
    "            training_source = output_ts[:, :-1]\n",
    "            # start symbol can not ba data to train.\n",
    "            training_data = output_ts[:, 1:]\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            decoder_output, _, attention_weight = decoder(training_source, hs, h)\n",
    "\n",
    "            for j in range(decoder_output.size()[1]):\n",
    "                loss += criterion(decoder_output[:, j, :], training_data[:, j])\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "        print(\"Epoch %d: %.2f\" % (epoch, epoch_loss))\n",
    "        all_losses.append(epoch_loss)\n",
    "        if epoch_loss < 0.1: break\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size, embedding_dim, hidden_dim, num_layers=1).to(device)\n",
    "decoder = AttentionDecoder(vocab_size, embedding_dim, hidden_dim, batch_size=BATCH_NUM).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Epoch 1: 1528.55\n",
      "Epoch 2: 36.01\n",
      "Epoch 3: 15.01\n",
      "Epoch 4: 6.91\n",
      "Epoch 5: 17.26\n",
      "Epoch 6: 6.02\n",
      "Epoch 7: 2.16\n",
      "Epoch 8: 1.32\n",
      "Epoch 9: 0.92\n",
      "Epoch 10: 0.68\n",
      "Epoch 11: 0.52\n",
      "Epoch 12: 0.40\n",
      "Epoch 13: 0.32\n",
      "Epoch 14: 0.26\n",
      "Epoch 15: 0.21\n",
      "Epoch 16: 0.17\n",
      "Epoch 17: 0.14\n",
      "Epoch 18: 0.11\n",
      "Epoch 19: 0.09\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "all_losses = train(EPOCH_NUM=100, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7b71656438>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_index(decoder_output):  \n",
    "    results = []\n",
    "    for h in decoder_output:\n",
    "        results.append(torch.argmax(h))\n",
    "        \n",
    "    results = torch.tensor(results, device=device).view(BATCH_NUM, 1)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>answer</th>\n",
       "      <th>predict</th>\n",
       "      <th>judge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8/28/73</td>\n",
       "      <td>1973-08-28</td>\n",
       "      <td>1973-08-28</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mar 1, 1976</td>\n",
       "      <td>1976-03-01</td>\n",
       "      <td>1976-03-01</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/8/98</td>\n",
       "      <td>1998-01-08</td>\n",
       "      <td>1998-01-08</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JULY 4, 1994</td>\n",
       "      <td>1994-07-04</td>\n",
       "      <td>1994-07-04</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9/25/93</td>\n",
       "      <td>1993-09-25</td>\n",
       "      <td>1993-09-25</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           input      answer     predict judge\n",
       "0  8/28/73                        1973-08-28  1973-08-28     O\n",
       "1  Mar 1, 1976                    1976-03-01  1976-03-01     O\n",
       "2  1/8/98                         1998-01-08  1998-01-08     O\n",
       "3  JULY 4, 1994                   1994-07-04  1994-07-04     O\n",
       "4  9/25/93                        1993-09-25  1993-09-25     O"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_input_batch, test_output_batch = train2batch(test_x, test_y)\n",
    "input_tensor = torch.tensor(test_input_batch, device=device)\n",
    "\n",
    "predicts = []\n",
    "for i in range(len(test_input_batch)):\n",
    "    with torch.no_grad(): \n",
    "        hs, h = encoder(input_tensor[i])\n",
    "        decoder_hidden = h\n",
    "\n",
    "        # Add start symbol.\n",
    "        start_char_batch = [[char2id[\"_\"]] for _ in range(BATCH_NUM)]\n",
    "        decoder_input_tensor = torch.tensor(start_char_batch, device=device)\n",
    "        # store prediction.\n",
    "        batch_tmp = torch.zeros(100,1, dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(output_len - 1):\n",
    "            decoder_output, decoder_hidden, _ = decoder(decoder_input_tensor, hs, decoder_hidden)\n",
    "            # output(predicted character) will be next input.\n",
    "            decoder_input_tensor = get_max_index(decoder_output.squeeze())\n",
    "            batch_tmp = torch.cat([batch_tmp, decoder_input_tensor], dim=1)\n",
    "            # remove head element.\n",
    "            \n",
    "    predicts.append(batch_tmp[:,1:])\n",
    "\n",
    "id2char = {}\n",
    "for k, v in char2id.items():\n",
    "    id2char[v] = k\n",
    "\n",
    "row = []\n",
    "for i in range(len(test_input_batch)):\n",
    "    batch_input = test_input_batch[i]\n",
    "    batch_output = test_output_batch[i]\n",
    "    batch_predict = predicts[i]\n",
    "    for inp, output, predict in zip(batch_input, batch_output, batch_predict):\n",
    "        x = [id2char[idx] for idx in inp]\n",
    "        y = [id2char[idx] for idx in output[1:]]\n",
    "        p = [id2char[idx.item()] for idx in predict]\n",
    "\n",
    "        x_str = \"\".join(x)\n",
    "        y_str = \"\".join(y)\n",
    "        p_str = \"\".join(p)\n",
    "\n",
    "        judge = \"O\" if y_str == p_str else \"X\"\n",
    "        row.append([x_str, y_str, p_str, judge])\n",
    "        \n",
    "predict_df = pd.DataFrame(row, columns=[\"input\", \"answer\", \"predict\", \"judge\"])\n",
    "predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>answer</th>\n",
       "      <th>predict</th>\n",
       "      <th>judge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [input, answer, predict, judge]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(predict_df.query('judge == \"O\"')) / len(predict_df))\n",
    "predict_df.query('judge == \"X\"').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
